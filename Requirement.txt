aiohttp==3.11.12
aiosignal==1.3.2
altair==5.5.0
annotated-types==0.7.0
anyio==4.8.0
astrapy==1.5.2
attrs==25.1.0
beautifulsoup4==4.13.3
blinker==1.9.0
cachetools==5.5.1
certifi==2025.1.31
charset-normalizer==3.4.1
click==8.1.8
ctransformers==0.2.27
dataclasses-json==0.6.7
deprecation==2.1.0
distro==1.9.0
dnspython==2.7.0
filelock==3.17.0
frozenlist==1.5.0
fsspec==2025.2.0
gitdb==4.0.12
GitPython==3.1.44
google==3.0.0
google-auth==2.38.0
google-genai==1.2.0
h11==0.14.0
h2==4.2.0
hpack==4.1.0
httpcore==1.0.7
httpx==0.28.1
httpx-sse==0.4.0
huggingface-hub==0.28.1
hyperframe==6.1.0
idna==3.10
Jinja2==3.1.5
jiter==0.8.2
jsonpatch==1.33
jsonpointer==3.0.0
jsonschema==4.23.0
jsonschema-specifications==2024.10.1
langchain==0.3.18
langchain-astradb==0.5.3
langchain-community==0.3.17
langchain-core==0.3.35
langchain-openai==0.3.5
langchain-text-splitters==0.3.6
langchainhub==0.1.21
langsmith==0.3.8
markdown-it-py==3.0.0
MarkupSafe==3.0.2
marshmallow==3.26.1
mdurl==0.1.2
multidict==6.1.0
mypy-extensions==1.0.0
narwhals==1.26.0
numpy==1.26.4
openai==1.62.0
orjson==3.10.15
packaging==24.2
pandas==2.2.3
pillow==11.1.0
propcache==0.2.1
protobuf==5.29.3
py-cpuinfo==9.0.0
pyarrow==19.0.0
pyasn1==0.6.1
pyasn1_modules==0.4.1
pydantic==2.10.6
pydantic-settings==2.7.1
pydantic_core==2.27.2
pydeck==0.9.1
Pygments==2.19.1
pymongo==4.11.1
python-dateutil==2.9.0.post0
python-dotenv==1.0.1
pytz==2025.1
PyYAML==6.0.2
referencing==0.36.2
regex==2024.11.6
requests==2.32.3
requests-toolbelt==1.0.0
rich==13.9.4
rpds-py==0.22.3
rsa==4.9
six==1.17.0
smmap==5.0.2
sniffio==1.3.1
soupsieve==2.6
SQLAlchemy==2.0.38
streamlit==1.42.0
tenacity==9.0.0
tiktoken==0.8.0
toml==0.10.2
tornado==6.4.2
tqdm==4.67.1
types-requests==2.32.0.20241016
typing-inspect==0.9.0
typing_extensions==4.12.2
tzdata==2025.1
urllib3==2.3.0
uuid6==2024.7.10
websockets==14.2
yarl==1.18.3
zstandard==0.23.0
unstructured==0.9.2
faiss-cpu==1.7.4
libmagic==1.0
python-magic==0.4.27
python-magic-bin==0.4.14
OpenAI == 0.28.0
[22/05, 7:32 am] Rabin âœ¨: import os
import streamlit as st
import pickle
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.vectorstores import FAISS
import google.generativeai as genai
from langchain_huggingface import HuggingFaceEmbeddings
import requests
from bs4 import BeautifulSoup
from langchain_community.llms import Ollama
from langchain.schema import Document
import ollama
 
client = ollama.Client()
model = "llama3.2"

st.title("RockyBot: News Research Tool ðŸ“ˆ")
st.sidebar.title("News Article URLs")

urls = []
for i in range(3):
    url = st.sidebar.text_input(f"URL {i+1}")
    urls.append(url)

process_url_clicked = st.sidebar.button("Process URLs")
file_path = "faiss_store_openai.pkl"
genai.configure(api_key="AIzaSyDtqWRImq2BFRgmtrRVA-urImwdSHOjgJ4")
main_placeholder = st.empty()
llm = Ollama(model="llama3.2")
# llm = genai.GenerativeModel(model_name="gemini-pro")


class MyError(Exception): 
    def _init_(self, value): 
        self.value = value 
    def _str_(self): 
        return(repr(self.value))
    

def get_text_from_url(url):
    response = requests.get(url)
    soup = BeautifulSoup(response.text, 'html.parser')
    paragraphs = soup.find_all('p')  
    text = ' '.join([p.text for p in paragraphs])  
    return Document(page_content=text)


if process_url_clicked:
    main_placeholder.text("Data Loading...Started...")
    data = [get_text_from_url(url) for url in urls]

    if not data:
        st.error("No data found in the provided URLs. Please check the URLs and try again.")
        raise (MyError("Some Error Data"))
    text_splitter = RecursiveCharacterTextSplitter(
        separators=['\n\n', '\n', '.', ','],
        chunk_size=1000
    )

    main_placeholder.text("Text Splitter...Started...")
    docs = text_splitter.split_documents(data)
    if not docs:
        st.error("No documents were split. Please check the content from the URLs.")
        raise (MyError("Some Error Data"))
    print("Documents after splitting:", docs)

    embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")

    try:
        if docs:
            vectorstore_openai = FAISS.from_documents(docs, embeddings)
            main_placeholder.text("Embedding Vector Started Building...")
        else:
            st.error("No valid documents to process into embeddings.")
            raise Exception()
    except Exception as e:
        st.error(f"Error during FAISS index creation: {str(e)}")
        raise Exception()
    with open(file_path, "wb") as f:
        pickle.dump(vectorstore_openai, f)


query = main_placeholder.text_input("Question: ")
if query:
    if os.path.exists(file_path):
        try:
            with open(file_path, "rb") as f:
                vectorstore = pickle.load(f)
                retriever = vectorstore.as_retriever()  
                retrieved_docs = retriever.get_relevant_documents(query) 
                retrieved_text = "\n".join([doc.page_content for doc in retrieved_docs])
                input_text = f"Answer the following question based on the documents:\n{retrieved_text}\n\nQuestion: {query}"
                response = client.generate(model=model,prompt=input_text)
                st.header("Answer")
                st.write(response.response)
        except Exception as e:

            st.error(f"Error during query processing: {str(e)}")