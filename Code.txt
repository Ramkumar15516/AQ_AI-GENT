import os
import streamlit as st
import pickle
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.vectorstores import FAISS
import google.generativeai as genai
from langchain_huggingface import HuggingFaceEmbeddings
import requests
from bs4 import BeautifulSoup
from langchain_community.llms import Ollama
from langchain.schema import Document
import ollama
 
client = ollama.Client()
model = "llama3.2"

st.title("RockyBot: News Research Tool ðŸ“ˆ")
st.sidebar.title("News Article URLs")

urls = []
for i in range(3):
    url = st.sidebar.text_input(f"URL {i+1}")
    urls.append(url)

process_url_clicked = st.sidebar.button("Process URLs")
file_path = "faiss_store_openai.pkl"
genai.configure(api_key="AIzaSyDtqWRImq2BFRgmtrRVA-urImwdSHOjgJ4")
main_placeholder = st.empty()
llm = Ollama(model="llama3.2")
# llm = genai.GenerativeModel(model_name="gemini-pro")


class MyError(Exception): 
    def _init_(self, value): 
        self.value = value 
    def _str_(self): 
        return(repr(self.value))
    

def get_text_from_url(url):
    response = requests.get(url)
    soup = BeautifulSoup(response.text, 'html.parser')
    paragraphs = soup.find_all('p')  
    text = ' '.join([p.text for p in paragraphs])  
    return Document(page_content=text)


if process_url_clicked:
    main_placeholder.text("Data Loading...Started...")
    data = [get_text_from_url(url) for url in urls]

    if not data:
        st.error("No data found in the provided URLs. Please check the URLs and try again.")
        raise (MyError("Some Error Data"))
    text_splitter = RecursiveCharacterTextSplitter(
        separators=['\n\n', '\n', '.', ','],
        chunk_size=1000
    )

    main_placeholder.text("Text Splitter...Started...")
    docs = text_splitter.split_documents(data)
    if not docs:
        st.error("No documents were split. Please check the content from the URLs.")
        raise (MyError("Some Error Data"))
    print("Documents after splitting:", docs)

    embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")

    try:
        if docs:
            vectorstore_openai = FAISS.from_documents(docs, embeddings)
            main_placeholder.text("Embedding Vector Started Building...")
        else:
            st.error("No valid documents to process into embeddings.")
            raise Exception()
    except Exception as e:
        st.error(f"Error during FAISS index creation: {str(e)}")
        raise Exception()
    with open(file_path, "wb") as f:
        pickle.dump(vectorstore_openai, f)


query = main_placeholder.text_input("Question: ")
if query:
    if os.path.exists(file_path):
        try:
            with open(file_path, "rb") as f:
                vectorstore = pickle.load(f)
                retriever = vectorstore.as_retriever()  
                retrieved_docs = retriever.get_relevant_documents(query) 
                retrieved_text = "\n".join([doc.page_content for doc in retrieved_docs])
                input_text = f"Answer the following question based on the documents:\n{retrieved_text}\n\nQuestion: {query}"
                response = client.generate(model=model,prompt=input_text)
                st.header("Answer")
                st.write(response.response)
        except Exception as e:
            st.error(f"Error during query processing: {str(e)}")